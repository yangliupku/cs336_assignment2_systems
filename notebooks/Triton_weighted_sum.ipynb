{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkOwQaX+aIz//fVhA36yoz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yangliupku/cs336_assignment2_systems/blob/main/notebooks/Triton_weighted_sum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5wImnWN9mtT-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "cvq2F5PbiOus"
      },
      "outputs": [],
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "import torch\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def weighted_sum_fwd(\n",
        "    x_ptr, weight_ptr,\n",
        "    output_ptr,\n",
        "    x_stride_row, x_stride_dim,\n",
        "    weight_stride_dim,\n",
        "    output_stride_row,\n",
        "    ROWS, D,\n",
        "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,\n",
        "):\n",
        "  row_tile_idx = tl.program_id(0)\n",
        "  tl.device_print(\"row_tile_idx\", row_tile_idx)\n",
        "\n",
        "  x_block_ptr = tl.make_block_ptr(\n",
        "      x_ptr,\n",
        "      shape=(ROWS, D),\n",
        "      strides=(x_stride_row, x_stride_dim),\n",
        "      offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
        "      block_shape = (ROWS_TILE_SIZE, D_TILE_SIZE),\n",
        "      order = (1, 0),\n",
        "  )\n",
        "\n",
        "  weight_block_ptr = tl.make_block_ptr(\n",
        "      weight_ptr,\n",
        "      shape=(D,),\n",
        "      strides=(weight_stride_dim,),\n",
        "      offsets=(0,),\n",
        "      block_shape=(D_TILE_SIZE,),\n",
        "      order=(0,),\n",
        "  )\n",
        "\n",
        "  output_block_ptr = tl.make_block_ptr(\n",
        "      output_ptr,\n",
        "      shape=(ROWS, ),\n",
        "      strides=(output_stride_row,),\n",
        "      offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
        "      block_shape=(ROWS_TILE_SIZE,),\n",
        "      order=(0,),\n",
        "  )\n",
        "\n",
        "  output = tl.zeros((ROWS_TILE_SIZE, ), dtype=tl.float32)\n",
        "\n",
        "  for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
        "    row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option='zero')\n",
        "    weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option='zero')\n",
        "    output += tl.sum(row*weight[None, :], axis=1)\n",
        "    x_block_ptr =x_block_ptr.advance((0, D_TILE_SIZE))\n",
        "    weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))\n",
        "\n",
        "  tl.store(output_block_ptr, output, boundary_check=(0,))\n",
        "\n",
        "class WeightedSumFunc(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, x, weight):\n",
        "    D, output_dims = x.shape[-1], x.shape[:-1]\n",
        "    input_shape = x.shape\n",
        "    x=rearrange(x, \"... d-> (...) d\")\n",
        "    ctx.save_for_backward(x, weight)\n",
        "    assert len(weight.shape) == 1 and weight.shape[0] == D\n",
        "    assert x.is_cuda and weight.is_cuda\n",
        "    assert x.is_contiguous()\n",
        "    ctx.D_TILE_SIZE = triton.next_power_of_2(D)\n",
        "    ctx.ROWS_TILE_SIZE = 16\n",
        "    ctx.input_shape = input_shape\n",
        "    y = torch.empty(output_dims, device = x.device)\n",
        "\n",
        "    n_rows = y.numel()\n",
        "    grid = (triton.cdiv(n_rows, ctx.ROWS_TILE_SIZE),)\n",
        "\n",
        "    weighted_sum_fwd[grid](\n",
        "        x, weight,\n",
        "        y,\n",
        "        x.stride(0), x.stride(1),\n",
        "        weight.stride(0),\n",
        "        y.stride(0),\n",
        "        ROWS=n_rows, D=D,\n",
        "        ROWS_TILE_SIZE=ctx.ROWS_TILE_SIZE, D_TILE_SIZE=ctx.D_TILE_SIZE,\n",
        "    )\n",
        "    return y.view(input_shape[:-1])\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    \"\"\"\n",
        "    grad_output is the gradient flowing backwards from whatever operation comes after your function.\n",
        "    It represents ∂Loss/∂y where y is your function's output.\n",
        "\n",
        "    Example: if your function outputs y with shape [5, 8], then:\n",
        "    grad_output has shape [5, 8] and contains ∂Loss/∂y\n",
        "    \"\"\"\n",
        "    x_flat, weight = ctx.saved_tensors\n",
        "    input_shape = ctx.input_shape\n",
        "\n",
        "    # Flatten grad_output to match x_flat\n",
        "    grad_output_flat = grad_output.contiguous().view(-1)\n",
        "\n",
        "    # Compute gradients\n",
        "    grad_x = None\n",
        "    grad_weight = None\n",
        "\n",
        "    if ctx.needs_input_grad[0]:\n",
        "        # grad_x = grad_output.unsqueeze(-1) * weight.unsqueeze(0)\n",
        "        grad_x = grad_output_flat.unsqueeze(-1) * weight.unsqueeze(0)\n",
        "        grad_x = grad_x.view(input_shape)\n",
        "\n",
        "    if ctx.needs_input_grad[1]:\n",
        "        # grad_weight = sum(grad_output.unsqueeze(-1) * x, dim=0)\n",
        "        grad_weight = torch.sum(grad_output_flat.unsqueeze(-1) * x_flat, dim=0)\n",
        "\n",
        "    return grad_x, grad_weight\n",
        "\n",
        "f_weighted_sum = WeightedSumFunc.apply\n"
      ],
      "metadata": {
        "id": "MFgVxwy0jS0k"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(5, 10, requires_grad=True, device='cuda')\n",
        "weight = torch.rand(10, requires_grad=True, device='cuda')\n",
        "y = WeightedSumFunc.apply(x, weight)\n",
        "print(y)\n",
        "y.sum().backward()\n",
        "print(x.grad)\n",
        "x.grad.data.zero_()\n",
        "weight.grad.data.zero_()\n",
        "z = (x*weight).sum(dim=-1)\n",
        "z.sum().backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htNBfRpMnLGD",
        "outputId": "b484115b-0e37-4f28-ce3b-6a8f6ee0d6a7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.7370, 2.4548, 2.7630, 2.4602, 2.9581], device='cuda:0',\n",
            "       grad_fn=<WeightedSumFuncBackward>)\n",
            "tensor([[0.5056, 0.4949, 0.1311, 0.5317, 0.7577, 0.0072, 0.6537, 0.9445, 0.2022,\n",
            "         0.1763],\n",
            "        [0.5056, 0.4949, 0.1311, 0.5317, 0.7577, 0.0072, 0.6537, 0.9445, 0.2022,\n",
            "         0.1763],\n",
            "        [0.5056, 0.4949, 0.1311, 0.5317, 0.7577, 0.0072, 0.6537, 0.9445, 0.2022,\n",
            "         0.1763],\n",
            "        [0.5056, 0.4949, 0.1311, 0.5317, 0.7577, 0.0072, 0.6537, 0.9445, 0.2022,\n",
            "         0.1763],\n",
            "        [0.5056, 0.4949, 0.1311, 0.5317, 0.7577, 0.0072, 0.6537, 0.9445, 0.2022,\n",
            "         0.1763]], device='cuda:0')\n",
            "tensor([[0.5056, 0.4949, 0.1311, 0.5317, 0.7577, 0.0072, 0.6537, 0.9445, 0.2022,\n",
            "         0.1763],\n",
            "        [0.5056, 0.4949, 0.1311, 0.5317, 0.7577, 0.0072, 0.6537, 0.9445, 0.2022,\n",
            "         0.1763],\n",
            "        [0.5056, 0.4949, 0.1311, 0.5317, 0.7577, 0.0072, 0.6537, 0.9445, 0.2022,\n",
            "         0.1763],\n",
            "        [0.5056, 0.4949, 0.1311, 0.5317, 0.7577, 0.0072, 0.6537, 0.9445, 0.2022,\n",
            "         0.1763],\n",
            "        [0.5056, 0.4949, 0.1311, 0.5317, 0.7577, 0.0072, 0.6537, 0.9445, 0.2022,\n",
            "         0.1763]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwsXJcv77ncn",
        "outputId": "558c33fd-0656-4a1b-fd59-38fb95a140ee"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9127, 0.6412, 0.9072, 0.7362, 0.2823, 0.8845, 0.2758, 0.0404, 0.5012,\n",
              "        0.5784], device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight.unsqueeze(0).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXvK338H_NXi",
        "outputId": "8cf0696f-a0c1-45c4-93c9-7477c3128167"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qSZ6N1la_P-G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}